{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03e7fa63-3ebf-49a9-b3ab-5f46dbf0967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61eba8d-4d25-4829-b74a-c9070fb33d09",
   "metadata": {},
   "source": [
    "## Loading the data for model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "659a94c2-7a59-4a99-b3a2-a5a24cd4202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eval_1_analysis.json', 'r') as f:\n",
    "        data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0154ed3-81cb-4f16-a572-65202e3d14c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': 29, 'gender': 'Male', 'ethnicity': 'white'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"78061\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f963a38e-fd83-4cc1-bfea-ebb76ac38746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairs(data):\n",
    "\n",
    "    target_ethnicities = ['indian', 'white', 'black', 'middle eastern', 'asian']\n",
    "    pairs_per_ethnicity = 150\n",
    "    identities_root = '/mnt/object/dataset/datasets/post_training_opt'\n",
    "\n",
    "    # Initialize separate lists for different categories\n",
    "    pairs = []\n",
    "    indian_pairs = []\n",
    "    white_pairs = []\n",
    "    black_pairs = []\n",
    "    middle_eastern_pairs = []\n",
    "    asian_pairs = []\n",
    "    male_pairs = []\n",
    "    female_pairs = []\n",
    "\n",
    "    # Prepare: group eligible identities by ethnicity\n",
    "    eligible = {eth: [] for eth in target_ethnicities}\n",
    "    for identity, info in data.items():\n",
    "        eth = info['ethnicity'].lower()\n",
    "        if eth in eligible:    \n",
    "            eligible[eth].append((identity, info))\n",
    "\n",
    "    for eth in target_ethnicities:\n",
    "        random.shuffle(eligible[eth])\n",
    "        count = 0\n",
    "        for identity, info in eligible[eth]:\n",
    "            folder = os.path.join(identities_root, identity)\n",
    "            images = os.listdir(folder)\n",
    "            if len(images) < 3:  # Need at least 3 images (1 anchor, 2 positive)\n",
    "                continue\n",
    "            \n",
    "            # Select anchor and two positive images\n",
    "            anchor_img = random.choice(images)\n",
    "            remaining_images = [img for img in images if img != anchor_img]\n",
    "            positive_imgs = random.sample(remaining_images, 2)\n",
    "            \n",
    "            # Find two negatives\n",
    "            candidates = [\n",
    "                (neg_id, neg_info) for neg_id, neg_info in eligible[eth]\n",
    "                if neg_id != identity and neg_info['gender'] == info['gender']\n",
    "            ]\n",
    "            if len(candidates) < 2:\n",
    "                continue\n",
    "                \n",
    "            negative_identities = random.sample(candidates, 2)\n",
    "            current_pairs = []\n",
    "            \n",
    "            for i in range(2):\n",
    "                anchor_path = os.path.join(folder, anchor_img)\n",
    "                positive_path = os.path.join(folder, positive_imgs[i])\n",
    "                \n",
    "                neg_identity, _ = negative_identities[i]\n",
    "                neg_folder = os.path.join(identities_root, neg_identity)\n",
    "                neg_images = os.listdir(neg_folder)\n",
    "                if not neg_images:\n",
    "                    continue\n",
    "                negative_img = random.choice(neg_images)\n",
    "                negative_path = os.path.join(neg_folder, negative_img)\n",
    "                \n",
    "                pair = (anchor_path, positive_path, negative_path, identity, neg_identity)\n",
    "                current_pairs.append(pair)\n",
    "            \n",
    "            if len(current_pairs) == 2:\n",
    "                pairs.extend(current_pairs)\n",
    "                # Add to specific ethnicity lists\n",
    "                if eth == 'indian':\n",
    "                    indian_pairs.extend(current_pairs)\n",
    "                elif eth == 'white':\n",
    "                    white_pairs.extend(current_pairs)\n",
    "                elif eth == 'black':\n",
    "                    black_pairs.extend(current_pairs)\n",
    "                elif eth == 'middle eastern':\n",
    "                    middle_eastern_pairs.extend(current_pairs)\n",
    "                elif eth == 'asian':\n",
    "                    asian_pairs.extend(current_pairs)\n",
    "                    \n",
    "                # Add to gender-specific lists\n",
    "                if info['gender'] == 'Male':\n",
    "                    male_pairs.extend(current_pairs)\n",
    "                else:\n",
    "                    female_pairs.extend(current_pairs)\n",
    "                    \n",
    "                count += 1\n",
    "                if count >= pairs_per_ethnicity:\n",
    "                    break\n",
    "\n",
    "    return {\"all_pairs\": pairs, \"indian_pairs\": indian_pairs, \"white_pairs\": white_pairs,\n",
    "            \"black_pairs\": black_pairs, \"middle_eastern_pairs\": middle_eastern_pairs,\n",
    "            \"asian_pairs\": asian_pairs, \"male_pairs\": male_pairs, \"female_pairs\": female_pairs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf72a10f-1e5c-456a-9fce-ba5436739889",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pairs(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48cabb7d-d69a-435e-80e9-8b7b510b7c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1314"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data[\"all_pairs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e69c47c5-cb6e-4ad5-9e9d-7527d67b21e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_100 = all_data[\"all_pairs\"][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d787149-e4e2-44b4-8cdc-7077e0075330",
   "metadata": {},
   "source": [
    "### Seeing if model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "780be6da-66c2-40c8-b1e3-0c6d2a81c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backbones.iresnet import iresnet100  # or your custom path if you cloned insightface locally\n",
    "\n",
    "# Load YAML\n",
    "# with open('ms1mv3_arcface_r100_fp16/model.yaml', 'r') as f:\n",
    "#     config = yaml.safe_load(f)\n",
    "\n",
    "# Create model\n",
    "# Try loading a state_dict first, otherwise assume the file *is* the model\n",
    "MODEL_PATH = \"backbone.pth\"\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=\"cpu\")\n",
    "if isinstance(checkpoint, dict):\n",
    "    model = iresnet100(pretrained=False)\n",
    "    model.load_state_dict(checkpoint)\n",
    "else:\n",
    "    model = checkpoint\n",
    "\n",
    "model.eval()\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def preprocess(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Image not found at {img_path}\")\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (112, 112))\n",
    "    img = np.transpose(img, (2, 0, 1))  # Channels first: (3, 112, 112)\n",
    "    img = img.astype(np.float32)\n",
    "    img = (img / 255.0 - 0.5) / 0.5  # Normalize\n",
    "    img = torch.from_numpy(img)      # <-- CONVERT numpy -> torch.Tensor\n",
    "    img = img.unsqueeze(0)           # Add batch dimension: (1, 3, 112, 112)\n",
    "    return img, img_rgb\n",
    "\n",
    "\n",
    "# Step 4: Inference function\n",
    "def get_embedding(img_path):\n",
    "    img_tensor, rgb = preprocess(img_path)\n",
    "    with torch.no_grad():\n",
    "        emb = model(img_tensor)\n",
    "        emb = F.normalize(emb, p=2, dim=1)\n",
    "    return emb, rgb\n",
    "\n",
    "# Step 5: Compare two embeddings\n",
    "def compare_embeddings(emb1, emb2, threshold=0.5):\n",
    "    similarity = F.cosine_similarity(emb1, emb2).item()\n",
    "    print(f\"Cosine Similarity: {similarity:.4f}\")\n",
    "    if similarity > threshold:\n",
    "        print(\"Result: SAME PERSON ✅\")\n",
    "    else:\n",
    "        print(\"Result: DIFFERENT PERSON ❌\")\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def plot_images(img1, img2, similarity):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    axs[0].imshow(img1)\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title('Image 1')\n",
    "\n",
    "    axs[1].imshow(img2)\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title('Image 2')\n",
    "\n",
    "    plt.suptitle(f'Cosine Similarity: {similarity:.4f}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd19794-1dcf-4d35-b065-1950eccf181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "five_pairs = random.sample(first_100, 1)\n",
    "\n",
    "num_samples = 0\n",
    "for pair in first_100:\n",
    "\n",
    "    embedding_anchor, rgb1 = get_embedding(pair[0])\n",
    "    embedding_pos, rgb2 = get_embedding(pair[1])\n",
    "    embedding_neg, rgb3 = get_embedding(pair[2])\n",
    "\n",
    "    print(\"Positive pair:\")\n",
    "    plot_images(rgb1, rgb2, compare_embeddings(embedding_anchor, embedding_pos))\n",
    "    print(\"Negative pair:\")\n",
    "    plot_images(rgb1, rgb3, compare_embeddings(embedding_anchor, embedding_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8f79aa73-80b5-4a95-92d1-4c4026469072",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = os.path.getsize('backbone.pth') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4273ff91-fe14-4b5e-bfda-f95c633daa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_100 = all_data[\"all_pairs\"][:100]\n",
    "first_100 = first_100[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a25f3a4-bdfa-41c1-aafa-1d493258b0c2",
   "metadata": {},
   "source": [
    "Each data entry has 2 samples one [pstive pair and one negative pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c64f294-229a-427d-95ef-9f00527d9574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04312b4b-1743-4d83-b43b-a6482e3ce377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def preprocess_batch(img_paths, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Load & normalize a list of image files into a tensor\n",
    "      shape (N, 3, 112, 112) on `device`.\n",
    "    \"\"\"\n",
    "    tensors = []\n",
    "    for p in img_paths:\n",
    "        img = cv2.imread(p)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Image not found: {p}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (112, 112))\n",
    "        img = img.astype(np.float32)\n",
    "        img = (img / 255.0 - 0.5) / 0.5\n",
    "        t = torch.from_numpy(img).permute(2, 0, 1)  # HWC → CHW\n",
    "        tensors.append(t)\n",
    "    batch = torch.stack(tensors, dim=0).to(device)  # (N,3,112,112)\n",
    "    return batch\n",
    "\n",
    "def embed_pair(anchor_path, other_path, model, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Batch the anchor & other image (2 images), run through model,\n",
    "    return (anchor_emb, other_emb) as two (D,) tensors.\n",
    "    \"\"\"\n",
    "    batch = preprocess_batch([anchor_path, other_path], device)  # (2,3,112,112)\n",
    "    model.to(device).eval()\n",
    "    with torch.no_grad():\n",
    "        embs = model(batch)                   # (2, D)\n",
    "        embs = F.normalize(embs, p=2, dim=1)  # L2-norm\n",
    "    return embs[0], embs[1]\n",
    "\n",
    "def batch_process_triplets(triplets, model, batch_size=32, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Given a list of (anchor, positive, negative) triplets,\n",
    "    returns three tensors of shape (N, D): (emb_anchors, emb_positives, emb_negatives).\n",
    "    Uses one forward per group (anchors, positives, negatives).\n",
    "    \"\"\"\n",
    "    model.to(device).eval()\n",
    "\n",
    "    # split file lists\n",
    "    anchors  = [a for a, p, n in triplets]\n",
    "    positives= [p for a, p, n in triplets]\n",
    "    negatives= [n for a, p, n in triplets]\n",
    "\n",
    "    def embed_list(paths):\n",
    "        \"\"\"Helper: embed in sub-batches and concat.\"\"\"\n",
    "        embs = []\n",
    "        for i in range(0, len(paths), batch_size):\n",
    "            batch = preprocess_batch(paths[i : i + batch_size], device)\n",
    "            with torch.no_grad():\n",
    "                e = model(batch)\n",
    "                e = F.normalize(e, p=2, dim=1)\n",
    "            embs.append(e)\n",
    "        return torch.cat(embs, dim=0)\n",
    "\n",
    "    emb_a = embed_list(anchors)\n",
    "    emb_p = embed_list(positives)\n",
    "    emb_n = embed_list(negatives)\n",
    "\n",
    "    return emb_a, emb_p, emb_n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792604e6-27a7-451d-afdd-efbf66cc6a20",
   "metadata": {},
   "source": [
    "### Single sample performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "35a0d77e-31ab-4914-9e52-86a34b635fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 0\n",
    "correct = 0\n",
    "latencies = []\n",
    "for pair in first_100:\n",
    "    start = time.time()\n",
    "    anchor_emb, pos_emb = embed_pair(pair[0], pair[1], model)\n",
    "    \n",
    "    sim_1 = F.cosine_similarity(anchor_emb.unsqueeze(0), pos_emb.unsqueeze(0), dim = 1).item()\n",
    "    if sim_1 >= 0.5:\n",
    "        correct += 1\n",
    "    num_samples +=1\n",
    "    end = time.time()\n",
    "    latencies.append(end-start)\n",
    "\n",
    "    start = time.time()\n",
    "    _, neg_emb = embed_pair(pair[0], pair[2], model)\n",
    "    \n",
    "    sim_2 = F.cosine_similarity(anchor_emb.unsqueeze(0), neg_emb.unsqueeze(0), dim = 1).item()\n",
    "    if sim_2 < 0.5:\n",
    "        correct += 1\n",
    "    num_samples +=1\n",
    "\n",
    "    end = time.time()\n",
    "    latencies.append(end-start)\n",
    "acc = correct/num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d05fb3f7-6fe9-4b80-9254-102a41f4a3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 261.22 MB\n",
      "Accuracy: 94.50% (189/200 correct)\n",
      "Inference Latency (single sample, median): 808.84 ms\n",
      "Inference Latency (single sample, 95th percentile): 1148.49 ms\n",
      "Inference Latency (single sample, 99th percentile): 1165.14 ms\n",
      "Inference Throughput (single sample): 1.13 FPS\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model Size on Disk: {model_size/ (1e6) :.2f} MB\")\n",
    "print(f\"Accuracy: {acc*100:.2f}% ({correct}/{num_samples} correct)\")\n",
    "print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Inference Throughput (single sample): {num_samples/np.sum(latencies):.2f} FPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb24476e-a457-48bc-a135-b1605e1bbb6f",
   "metadata": {},
   "source": [
    "### Batch throughput and latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "65e598ff-9f7f-4b5a-9576-8d8ac9fee880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 10 batches; first batch has 10 triplets\n"
     ]
    }
   ],
   "source": [
    "#Creating batches\n",
    "\n",
    "def chunk_triplets(triplets, batch_size=10):\n",
    "    \"\"\"\n",
    "    Yield successive batches of `batch_size` triplets from the list.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(triplets), batch_size):\n",
    "        yield triplets[i : i + batch_size]\n",
    "\n",
    "# Example: turn first_100 into a list of 10‐triplet batches\n",
    "batches = list(chunk_triplets(first_100, batch_size=10))\n",
    "print(f\"Got {len(batches)} batches; first batch has {len(batches[0])} triplets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9b56046c-3ec5-4696-bd55-7e65fdcc9832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate_triplets_batch(triplets, model, threshold=0.5, batch_size=32, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    triplets: list of (anchor_path, positive_path, negative_path)\n",
    "    model:    your loaded face‐embedding model\n",
    "    threshold: similarity threshold\n",
    "    batch_size: sub‐batch size for GPU/CPU\n",
    "    device:   \"cpu\" or \"cuda\"\n",
    "    \"\"\"\n",
    "    # 1) Warm up & start timer\n",
    "    model.to(device).eval()\n",
    "    start = time.time()\n",
    "\n",
    "    # 2) Embed all anchors, positives, negatives in big batches\n",
    "    emb_a, emb_p, emb_n = batch_process_triplets(\n",
    "        triplets, model, batch_size=batch_size, device=device\n",
    "    )  # each is (N, D)\n",
    "\n",
    "    # 3) Cosine‐similarities\n",
    "    sims_pos = F.cosine_similarity(emb_a, emb_p, dim=1)  # (N,)\n",
    "    sims_neg = F.cosine_similarity(emb_a, emb_n, dim=1)  # (N,)\n",
    "\n",
    "    # 4) Compute metrics\n",
    "    N = len(triplets)\n",
    "    num_samples = 2 * N\n",
    "    correct = int((sims_pos >= threshold).sum().item() +\n",
    "                  (sims_neg <  threshold).sum().item())\n",
    "\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    throughput = num_samples / time_taken\n",
    "    accuracy   = correct / num_samples\n",
    "    latency    = time_taken / num_samples\n",
    "\n",
    "    # 5) Print exactly like your original loop\n",
    "    print(\"Time taken is \", time_taken)\n",
    "    print(\"Accuracy is \", accuracy)\n",
    "    print(\"Throughput is \", throughput)\n",
    "    print(\"Single sample latency is on avg: \", latency)\n",
    "\n",
    "    return accuracy, throughput, latency\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# Example usage with your first_100[:10]:\n",
    "# acc, tput, lat = evaluate_triplets_batch(first_100[:10], model, threshold=0.5, batch_size=16, device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9fa135b3-7b41-4003-9f89-3a4d68917cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Evaluating batch #1\n",
      "Time taken is  11.812459230422974\n",
      "Accuracy is  0.85\n",
      "Throughput is  1.6931275367698222\n",
      "Single sample latency is on avg:  0.5906229615211487\n",
      "  Batch #1 → acc=0.850, throughput=1.7 img/s, latency=590.6 ms/img\n",
      "→ Evaluating batch #2\n",
      "Time taken is  11.662468671798706\n",
      "Accuracy is  1.0\n",
      "Throughput is  1.714902784550451\n",
      "Single sample latency is on avg:  0.5831234335899353\n",
      "  Batch #2 → acc=1.000, throughput=1.7 img/s, latency=583.1 ms/img\n",
      "→ Evaluating batch #3\n",
      "Time taken is  11.985369443893433\n",
      "Accuracy is  1.0\n",
      "Throughput is  1.6687011688396503\n",
      "Single sample latency is on avg:  0.5992684721946716\n",
      "  Batch #3 → acc=1.000, throughput=1.7 img/s, latency=599.3 ms/img\n",
      "→ Evaluating batch #4\n",
      "Time taken is  11.745134353637695\n",
      "Accuracy is  0.95\n",
      "Throughput is  1.7028327984860907\n",
      "Single sample latency is on avg:  0.5872567176818848\n",
      "  Batch #4 → acc=0.950, throughput=1.7 img/s, latency=587.3 ms/img\n",
      "→ Evaluating batch #5\n",
      "Time taken is  12.220762252807617\n",
      "Accuracy is  0.95\n",
      "Throughput is  1.6365591266948318\n",
      "Single sample latency is on avg:  0.6110381126403809\n",
      "  Batch #5 → acc=0.950, throughput=1.6 img/s, latency=611.0 ms/img\n",
      "→ Evaluating batch #6\n",
      "Time taken is  11.654233694076538\n",
      "Accuracy is  1.0\n",
      "Throughput is  1.7161145490128054\n",
      "Single sample latency is on avg:  0.582711684703827\n",
      "  Batch #6 → acc=1.000, throughput=1.7 img/s, latency=582.7 ms/img\n",
      "→ Evaluating batch #7\n",
      "Time taken is  11.592485427856445\n",
      "Accuracy is  0.95\n",
      "Throughput is  1.7252555652940924\n",
      "Single sample latency is on avg:  0.5796242713928222\n",
      "  Batch #7 → acc=0.950, throughput=1.7 img/s, latency=579.6 ms/img\n",
      "→ Evaluating batch #8\n",
      "Time taken is  11.953201293945312\n",
      "Accuracy is  0.95\n",
      "Throughput is  1.6731919347941253\n",
      "Single sample latency is on avg:  0.5976600646972656\n",
      "  Batch #8 → acc=0.950, throughput=1.7 img/s, latency=597.7 ms/img\n",
      "→ Evaluating batch #9\n",
      "Time taken is  11.85840892791748\n",
      "Accuracy is  0.9\n",
      "Throughput is  1.6865669013079234\n",
      "Single sample latency is on avg:  0.592920446395874\n",
      "  Batch #9 → acc=0.900, throughput=1.7 img/s, latency=592.9 ms/img\n",
      "→ Evaluating batch #10\n",
      "Time taken is  11.591006517410278\n",
      "Accuracy is  0.9\n",
      "Throughput is  1.7254756927242676\n",
      "Single sample latency is on avg:  0.5795503258705139\n",
      "  Batch #10 → acc=0.900, throughput=1.7 img/s, latency=579.6 ms/img\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "latencies = []\n",
    "num_batches = 0\n",
    "\n",
    "for idx, batch in enumerate(batches, start=1):\n",
    "    print(f\"→ Evaluating batch #{idx}\")\n",
    "    triplets = [ t[:3] for t in batch ]\n",
    "    acc, tput, lat = evaluate_triplets_batch(\n",
    "        triplets,\n",
    "        model,\n",
    "        threshold=0.5,\n",
    "        batch_size=10,   # you could pick a larger sub‐batch for GPU\n",
    "        device=\"cpu\"\n",
    "    )\n",
    "    accuracy += acc\n",
    "    latencies.append(lat)\n",
    "    throughput += tput\n",
    "    num_batches += 1\n",
    "\n",
    "\n",
    "    print(f\"  Batch #{idx} → acc={acc:.3f}, throughput={tput:.1f} img/s, latency={lat*1000:.1f} ms/img\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "92bd6a54-5587-4864-ab69-85f9ce0d3127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 261.22 MB\n",
      "Accuracy: 0.95%\n",
      "Inference Latency (single sample, median): 588.94 ms\n",
      "Inference Latency (single sample, 95th percentile): 605.74 ms\n",
      "Inference Latency (single sample, 99th percentile): 609.98 ms\n",
      "Batch Throughput: 1.87 FPS\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy/num_batches\n",
    "throughput = throughput/num_batches\n",
    "\n",
    "\n",
    "print(f\"Model Size on Disk: {model_size/ (1e6) :.2f} MB\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Batch Throughput: {throughput:.2f} FPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5aaf80-b682-40e2-b6c2-6e283fc6a933",
   "metadata": {},
   "source": [
    "## ONNX Impelmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da80339-986c-4de6-b4f0-b59cb623cfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "ONNX_PATH = \"backbone.onnx\"\n",
    "\n",
    "MODEL_PATH = \"backbone.pth\"\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=\"cpu\")\n",
    "if isinstance(checkpoint, dict):\n",
    "    model = iresnet100(pretrained=False)\n",
    "    model.load_state_dict(checkpoint)\n",
    "else:\n",
    "    model = checkpoint\n",
    "\n",
    "model.eval()\n",
    "\n",
    "dummy = torch.randn(1, 3, 112, 112, dtype=torch.float32)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy,\n",
    "    ONNX_PATH,\n",
    "    export_params=True,            # store the trained parameter weights inside the model file\n",
    "    opset_version=17,              # ONNX opset version\n",
    "    input_names=[\"input\"],         # model's input names\n",
    "    output_names=[\"output\"],       # model's output names\n",
    "    dynamic_axes={                 # allow variable batch size\n",
    "        \"input\":  {0: \"batch\"},\n",
    "        \"output\": {0: \"batch\"},\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"✅ ONNX model saved to {ONNX_PATH}\")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Load your ONNX model\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "ONNX_PATH = \"model_serve/fastapi_pt/iresnet100.onnx\"\n",
    "sess = ort.InferenceSession(ONNX_PATH, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 2) Preprocessing → NumPy batches\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def preprocess_batch(img_paths):\n",
    "    \"\"\"\n",
    "    Read & normalize a list of image files to a NumPy array\n",
    "    of shape (N, 3, 112, 112), dtype float32.\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    for p in img_paths:\n",
    "        img = cv2.imread(p)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Image not found: {p}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (112, 112))\n",
    "        img = img.astype(np.float32)\n",
    "        img = (img / 255.0 - 0.5) / 0.5\n",
    "        # HWC → CHW\n",
    "        t = np.transpose(img, (2, 0, 1))\n",
    "        batches.append(t)\n",
    "    return np.stack(batches, axis=0)  # (N,3,112,112)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 3) Single‐pair embed via ONNX\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def embed_pair_onnx(anchor_path, other_path):\n",
    "    \"\"\"\n",
    "    Batch the anchor & other image → call ONNX → return two (D,) arrays.\n",
    "    \"\"\"\n",
    "    batch_np = preprocess_batch([anchor_path, other_path])  # (2,3,112,112)\n",
    "    out = sess.run(None, {\"input\": batch_np})[0]            # (2, D)\n",
    "    # L2‐normalize each row\n",
    "    norms = np.linalg.norm(out, axis=1, keepdims=True)\n",
    "    embs = out / (norms + 1e-8)\n",
    "    return embs[0], embs[1]\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 4) Batch‐triplets embed via ONNX\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def batch_process_triplets_onnx(triplets, batch_size=32):\n",
    "    \"\"\"\n",
    "    Given list of (anchor, pos, neg) paths, returns three arrays\n",
    "    of shape (N, D): (emb_a, emb_p, emb_n).\n",
    "    \"\"\"\n",
    "    anchors   = [a for a, p, n in triplets]\n",
    "    positives = [p for a, p, n in triplets]\n",
    "    negatives = [n for a, p, n in triplets]\n",
    "\n",
    "    def embed_list(paths):\n",
    "        parts = []\n",
    "        for i in range(0, len(paths), batch_size):\n",
    "            sub = paths[i : i + batch_size]\n",
    "            batch_np = preprocess_batch(sub)            # (B,3,112,112)\n",
    "            out = sess.run(None, {\"input\": batch_np})[0]  # (B,D)\n",
    "            norms = np.linalg.norm(out, axis=1, keepdims=True)\n",
    "            parts.append(out / (norms + 1e-8))\n",
    "        return np.vstack(parts)\n",
    "\n",
    "    emb_a = embed_list(anchors)\n",
    "    emb_p = embed_list(positives)\n",
    "    emb_n = embed_list(negatives)\n",
    "    return emb_a, emb_p, emb_n\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 5) Chunk your triplets into batches of n\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def chunk_triplets(triplets, batch_size=10):\n",
    "    for i in range(0, len(triplets), batch_size):\n",
    "        yield triplets[i : i + batch_size]\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 6) Evaluate one batch of triplets\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def evaluate_triplets_batch_onnx(triplets, threshold=0.5, batch_size=32):\n",
    "    \"\"\"\n",
    "    triplets: list of (anchor, pos, neg)\n",
    "    Returns (accuracy, throughput_FPS, avg_latency_s) for that batch.\n",
    "    \"\"\"\n",
    "    N = len(triplets)\n",
    "    if N == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    t0 = time.time()\n",
    "    emb_a, emb_p, emb_n = batch_process_triplets_onnx(triplets, batch_size)\n",
    "    sims_pos = np.sum(emb_a * emb_p, axis=1)  # cosine = dot since L2-normed\n",
    "    sims_neg = np.sum(emb_a * emb_n, axis=1)\n",
    "\n",
    "    correct = np.count_nonzero(sims_pos >= threshold) + \\\n",
    "              np.count_nonzero(sims_neg < threshold)\n",
    "    total   = 2 * N\n",
    "\n",
    "    t1 = time.time()\n",
    "    elapsed = t1 - t0\n",
    "    throughput = total / elapsed\n",
    "    avg_latency = elapsed / total\n",
    "    accuracy    = correct / total\n",
    "\n",
    "    print(f\"Batch of {N} triplets → time: {elapsed:.3f}s, \"\n",
    "          f\"throughput: {throughput:.1f} img/s, \"\n",
    "          f\"latency: {avg_latency*1000:.1f}ms/img, \"\n",
    "          f\"acc: {accuracy*100:.2f}% ({correct}/{total})\")\n",
    "    return accuracy, throughput, avg_latency\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 7) Main loop: chunk into 10s and evaluate\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# Suppose first_100 is already defined: list of (a,p,n, …) tuples\n",
    "triplets_all = [t[:3] for t in first_100]  # discard extra fields\n",
    "\n",
    "model_size = os.path.getsize(ONNX_PATH)\n",
    "print(f\"Model Size on Disk: {model_size/1e6:.2f} MB\")\n",
    "\n",
    "all_acc = []\n",
    "all_throughput = []\n",
    "all_latencies = []\n",
    "\n",
    "for idx, batch in enumerate(chunk_triplets(triplets_all, batch_size=10), 1):\n",
    "    print(f\"\\n→ Evaluating batch #{idx}\")\n",
    "    acc, tput, lat = evaluate_triplets_batch_onnx(batch, threshold=0.5, batch_size=10)\n",
    "    all_acc.append(acc)\n",
    "    all_throughput.append(tput)\n",
    "    all_latencies.append(lat)\n",
    "\n",
    "# Aggregate over all batches\n",
    "mean_acc = np.mean(all_acc) * 100\n",
    "median_lat = np.median(all_latencies) * 1000\n",
    "p95_lat   = np.percentile(all_latencies, 95) * 1000\n",
    "p99_lat   = np.percentile(all_latencies, 99) * 1000\n",
    "mean_tput = np.mean(all_throughput)\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"Model Size on Disk: {model_size/1e6:.2f} MB\")\n",
    "print(f\"Accuracy: {mean_acc:.2f}%\")\n",
    "print(f\"Inference Latency (single sample, median): {median_lat:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 95th percentile): {p95_lat:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 99th percentile): {p99_lat:.2f} ms\")\n",
    "print(f\"Inference Throughput (single sample): {mean_tput:.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea69b27b-13f7-4e99-8c3f-2620583c9a41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
